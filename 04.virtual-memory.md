# 4. Virtual Memory 虚拟内存

The virtual memory subsystem of a processor implements the virtual address spaces provided to each process. This makes each process think it is alone in the system. The list of advantages of virtual memory are described in detail elsewhere so they will not be repeated here. Instead this section concentrates on the actual implementation details of the virtual memory subsystem and the associated costs.

一个处理器的虚拟内存(virtual memory,VM)子系统实现了提供给每个进程的虚拟地址空间.这令每个进程都认为它是独自在系统中的.虚拟内存的优点清单已经在其他地方描述过,所以这里就一一赘述.这一节会聚焦在虚拟内存子系统的实现细节,以及与此相关的开销.

A virtual address space is implemented by the Memory Management Unit (MMU) of the CPU. The OS has to fill out the page table data structures, but most CPUs do the rest of the work themselves. This is actually a pretty complicated mechanism; the best way to understand it is to introduce the data structures used to describe the virtual address space.

虚拟地址空间是由CPU 的内存管理单元(*Memory Management Unit*,MMU)实现的.OS 必须填写页表(*page table*)数据结构,大多数CPU 会自行做掉剩下的工作.这真的是个非常复杂的机制;理解它的最佳方式是介绍描述虚拟地址空间的数据结构.

The input to the address translation performed by the MMU is a virtual address. There are usually few—if any—restrictions on its value. Virtual addresses are 32-bit values on 32-bit systems, and 64-bit values on 64-bit systems. On some systems, for instance x86 and x86-64, the addresses used actually involve another level of indirection: these architectures use segments which simply cause an offset to be added to every logical address. We can ignore this part of address generation, it is trivial and not something that programmers have to care about with respect to performance of memory handling.

由MMU 实行的地址翻译的输入为一个虚拟地址.它的值通常有极少量––如果有的话––的限制.在32 位系统上的虚拟地址为32 位的值,而在64 位系统上为64 位的值.在某些系统上,像是x86 与x86-64,使用的地址实际上牵涉到另一层级的间接性:这些架构使用了分段(segment),其只不过是将偏移量加到每个逻辑地址上.我们可以忽略地址产生过程的这个部分,它很琐碎,而且就内存管理的性能而言,不是程序员必须要关心的东西.[[24]](#24)

## 4.1. 最简单的地址翻译

The interesting part is the translation of the virtual address to a physical address. The MMU can remap addresses on a page-by-page basis. Just as when addressing cache lines, the virtual address is split into distinct parts. These parts are used to index into various tables which are used in the construction of the final physical address. For the simplest model we have only one level of tables.

有趣的部分是虚拟地址到物理地址的翻译.MMU 能够逐个分页重新映射地址.就如同定址缓存行的时候一样,虚拟地址会被切成多个部分.这些部分用来索引多个用以建构最终物理地址的表.以最简单的模型而言,我们仅有一个层级的表.

![图4.1:一层地址翻译](assets/figure-4.1.png)

Figure 4.1 shows how the different parts of the virtual address are used. A top part is used to select an entry in a Page Directory; each entry in that directory can be individually set by the OS. The page directory entry determines the address of a physical memory page; more than one entry in the page directory can point to the same physical address. The complete physical address of the memory cell is determined by combining the page address from the page directory with the low bits from the virtual address. The page directory entry also contains some additional information about the page such as access permissions.

图4.1 显示了到底是怎么使用虚拟地址的不同部分的.开头的部分用以选择一个分页目录(Page Directory)中的一个项;在这个目录中的每个项都能由OS 个别设定.分页目录项决定了一个物理内存分页的地址;在分页目录中,能够有多于一个指到相同物理地址的项.记忆单元的完整物理地址是由分页目录的分页地址,结合虚拟地址的低位所决定的.分页目录项也包含一些像是存取权限这类关于分页的额外资讯.

The data structure for the page directory is stored in memory. The OS has to allocate contiguous physical memory and store the base address of this memory region in a special register. The appropriate bits of the virtual address are then used as an index into the page directory, which is actually an array of directory entries.

分页目录的数据结构储存于主存中.OS 必须分配连续的物理内存,并将这个内存区域的基底地址(base address)储存在一个特殊的寄存器中.虚拟内存中适当的位量接着会被用作一个分页目录的索引––它实际上是一个目录项的数组.

For a concrete example, this is the layout used for 4MB pages on x86 machines. The Offset part of the virtual address is 22 bits in size, enough to address every byte in a 4MB page. The remaining 10 bits of the virtual address select one of the 1024 entries in the page directory. Each entry contains a 10 bit base address of a 4MB page which is combined with the offset to form a complete 32 bit address.

作为一个实际的例子,以下是在x86 机器上的4MB 分页所使用的布局.虚拟内存的偏移量部分的大小为22 位,足以定址一个4MB 分页中的每个字节.虚拟内存剩余的10 位选择了分页目录里1024 个项中的其中一个.每个项包含一个4MB 分页的一个10 位的基底地址,其会与偏移量结合以构成完整的32 位地址.

## 4.2. 多层级页表

4MB pages are not the norm, they would waste a lot of memory since many operations an OS has to perform require alignment to memory pages. With 4kB pages (the norm on 32-bit machines and, still, often on 64-bit machines), the Offset part of the virtual address is only 12 bits in size. This leaves 20 bits as the selector of the page directory. A table with 220 entries is not practical. Even if each entry would be only 4 bytes the table would be 4MB in size. With each process potentially having its own distinct page directory much of the physical memory of the system would be tied up for these page directories.

4MB 的分页并非常态,它们会浪费很多的内存,因为OS 必须执行的许多操作都需要与内存分页对齐(align).以4kB 分页而言(32 位机器,甚至经常是64 位机器上的常态),虚拟地址的偏移量部分的大小仅有12 位.这留了20 位作为分页目录的选择器.一个有着2^20个项的表是不切实际的.即使每个项只会有4 字节,表大小也会有4MB.由于每个进程都可能拥有它自己独有的分页目录,这些分页目录会占据系统中大量的物理内存.

The solution is to use multiple levels of page tables. These can then represent a sparse huge page directory where regions which are not actually used do not require allocated memory. The representation is therefore much more compact, making it possible to have the page tables for many processes in memory without impacting performance too much.

解决方法是使用多个层级的页表.层次于是形成一个巨大,稀疏的分页目录;没有真的用到的地址空间范围不需要被分配的内存.这种表示法因而紧密得多了,使得内存中能够拥有许多进程的页表,而不会太过于影响性能.

![图4.2:四层地址翻译](assets/figure-4.2.png)

Today the most complicated page table structures comprise four levels. Figure 4.2 shows the schematics of such an implementation.

现今最复杂的页表结构由四个层级所构成.图4.2 显示了这种实现的示意图.

The virtual address is, in this example, split into at least five parts. Four of these parts are indexes into the various directories. The level 4 directory is referenced using a special-purpose register in the CPU. The content of the level 4 to level 2 directories is a reference to next lower level directory. If a directory entry is marked empty it obviously need not point to any lower directory. This way the page table tree can be sparse and compact. The entries of the level 1 directory are, just like in Figure 4.1, partial physical addresses, plus auxiliary data like access permissions.

虚拟内存––在这个例子中––被切成至少五个部分.其中四个部分为不同目录的索引.第四层目录被CPU 中一种特殊用途的寄存器所指向.第二层到第四层目录的内容为指向更低层级目录的参考.若是一个目录项被标记为空,它显然不需要指到任何更低层的目录.如此一来,页表树便能够稀疏且紧密.第一层目录的项为––就像在图4.1 一样––部分的物理地址,加上像存取权限这类辅助数据.

To determine the physical address corresponding to a virtual address the processor first determines the address of the highest level directory. This address is usually stored in a register. Then the CPU takes the index part of the virtual address corresponding to this directory and uses that index to pick the appropriate entry. This entry is the address of the next directory, which is indexed using the next part of the virtual address. This process continues until it reaches the level 1 directory, at which point the value of the directory entry is the high part of the physical address. The physical address is completed by adding the page offset bits from the virtual address. This process is called page tree walking. Some processors (like x86 and x86-64) perform this operation in hardware, others need assistance from the OS.

要确定对应到一个虚拟地址的物理地址,处理器首先会确定最高层目录的地址.这个地址通常储存在一个寄存器中.CPU 接着取出对应到这个目录的虚拟内存的索引部分,并使用这个索引来挑选合适的项.这个项是下一个目录的地址,使用虚拟地址的下一个部分来索引.这个过程持续到抵达第一层目录,这时目录项的值为物理地址的高位部分.加上来自虚拟内存的分页偏移位便组成了完整的物理地址.这个过程被称为分页树走访(page tree walking).有些处理器(像是x86 与x86-64)会在硬件中执行这个操作,其它的则需要来自OS 的协助.

Each process running on the system might need its own page table tree. It is possible to partially share trees but this is rather the exception. It is therefore good for performance and scalability if the memory needed by the page table trees is as small as possible. The ideal case for this is to place the used memory close together in the virtual address space; the actual physical addresses used do not matter. A small program might get by with using just one directory at each of levels 2, 3, and 4 and a few level 1 directories. On x86-64 with 4kB pages and 512 entries per directory this allows the addressing of 2MB with a total of 4 directories (one for each level). 1GB of contiguous memory can be addressed with one directory for levels 2 to 4 and 512 directories for level 1.

每个在系统中执行的进程会需要它自己的页表树.部分地共享树是可能的,但不如说这是个例外状况.因此,如果页表树所需的内存尽可能地小的话,对性能与延展性而言都是有益的.理想的情况是将用到的内存彼此靠近地摆在虚拟地址空间中;实际用到的物理地址则无关紧要.对一个小程序而言,仅仅使用在第二,三,四层各自的一个目录,以及少许第一层目录,可能还过得去.在有着4kB 分页以及每目录512 个项的x86-64 上,这能够以总计4 个目录(每层一个)来定址2MB.1GB 的连续内存能够以一个第二到第四层目录,以及512 个第一层目录来定址.

Assuming all memory can be allocated contiguously is too simplistic, though. For flexibility reasons the stack and the heap area of a process are, in most cases, allocated at pretty much opposite ends of the address space. This allows either area to grow as much as possible if needed. This means that there are most likely two level 2 directories needed and correspondingly more lower level directories.

不过,假设能够连续地分配所有内存也太过于简化了.为了弹性起见,一个进程的栈(stack)与堆(heap)区域––在大多情况下––会被分配在地址空间中极为相对的两端.这令两个区域在需要时都能尽可能地增长.这表示,最有可能是两个需要的第二层目录,以及与此相应的,更多低层级的目录.

But even this does not always match current practice. For security reasons the various parts of an executable (code, data, heap, stack, DSOs, aka shared libraries) are mapped at randomized addresses [[9]](F.bibliography.md#9). The randomization extends to the relative position of the various parts; that implies that the various memory regions in use in a process are widespread throughout the virtual address space. By applying some limits to the number of bits of the address which are randomized the range can be restricted, but it certainly, in most cases, will not allow a process to run with just one or two directories for levels 2 and 3.

但即使如此也不总是符合当前的实际状况.为了安全考量,一个可执进程序的多个部分(代码,数据,堆,栈,动态共享对象(*Dynamic Shared Object*,DSO),又称共享函式库(*shared library*))会被映射在随机化的地址上[[参考资料9]](F.bibliography.md#9).随机化扩大了不同部分的相对位置;这暗示着,在一个进程里使用中的不同内存区域会广泛地散布在虚拟地址空间中.由在随机化的地址的位数上施加一些限制也能够限制范围,但这无疑––在大多情况下––会让一个进程无法以仅仅一或两个第二与第三层目录来执行.

If performance is really much more important than security, randomization can be turned off. The OS will then usually at least load all DSOs contiguously in virtual memory.

若是性能比起安全性真的重要太多了,也能够把随机化关闭.于是OS 通常至少会在虚拟内存中连续地载入所有的DSO.

## 4.3. 优化页表存取

All the data structures for the page tables are kept in the main memory; this is where the OS constructs and updates the tables. Upon creation of a process or a change of a page table the CPU is notified. The page tables are used to resolve every virtual address into a physical address using the page table walk described above. More to the point: at least one directory for each level is used in the process of resolving a virtual address. This requires up to four memory accesses (for a single access by the running process) which is slow. It is possible to treat these directory table entries as normal data and cache them in L1d, L2, etc., but this would still be far too slow.

页表的所有数据结构都会被保存在主存中;OS 就是在这里建构并更新表的.在一个进程的创建,或是页表的一次修改之后,都会立即通知CPU.页表是用以将每个虚拟地址,使用上述的页表走访来转成物理地址.更准确地说:每一层至少会有一个目录会被用在转换一个虚拟地址的过程中.这需要高达四次内存存取(以执行中进程的一个单一存取而言),这很慢.将这些目录表的项视为普通的数据,并在L1d,L2,等等缓存它们是办得到的,但这可能还是太慢了.

From the earliest days of virtual memory, CPU designers have used a different optimization. A simple computation can show that only keeping the directory table entries in the L1d and higher cache would lead to horrible performance. Each absolute address computation would require a number of L1d accesses corresponding to the page table depth. These accesses cannot be parallelized since they depend on the previous lookup's result. This alone would, on a machine with four page table levels, require at the very least 12 cycles. Add to that the probability of an L1d miss and the result is nothing the instruction pipeline can hide. The additional L1d accesses also steal precious bandwidth to the cache.

从最早期的虚拟内存开始,CPU 设计者便已采用了一种不同的优化.一个简单的计算能够显示,仅将目录表的项保存在L1d 以及更高层级的缓存中会招致可怕的性能.每个独立的地址计算会需要相符于页表深度的若干L1d 存取.这些存取无法并行化,因为它们都依赖于前一次查询的结果.单是这样就会––在一台有着四个页表层次的机器上––需要至少12 个周期.再加上L1d 不命中的机率,结果是指令管道无法隐藏任何东西.额外的L1d 存取也需要将宝贵的带宽偷到缓存去.

So, instead of just caching the directory table entries, the complete computation of the address of the physical page is cached. For the same reason that code and data caches work, such a cached address computation is effective. Since the page offset part of the virtual address does not play any part in the computation of the physical page address, only the rest of the virtual address is used as the tag for the cache. Depending on the page size this means hundreds or thousands of instructions or data objects share the same tag and therefore same physical address prefix.

所以,不只是将目录表的项缓存起来,而是连物理分页地址的完整计算结果也会被缓存.跟代码与数据缓存行得通的理由相同,这种缓存的地址计算结果是很有效的.由于虚拟地址的分页偏移量的部分不会参与到物理分页地址的计算,仅有虚拟地址的剩余部分会用来作为缓存的标签.视分页大小而定,这代表数百或数千条的指令或数据对象会共享相同的标签,因而共享相同的物理地址前缀(prefix).

The cache into which the computed values are stored is called the Translation Look-Aside Buffer (TLB). It is usually a small cache since it has to be extremely fast. Modern CPUs provide multi-level TLB caches, just as for the other caches; the higher-level caches are larger and slower. The small size of the L1TLB is often made up for by making the cache fully associative, with an LRU eviction policy. Recently, this cache has been growing in size and, in the process, was changed to be set associative. As a result, it might not be the oldest entry which gets evicted and replaced whenever a new entry has to be added.

储存计算得来的值的缓存被称为转换检测缓冲区(*Translation Look-Aside Buffer*,TLB).它通常是个很小的缓存,因为它必须非常快.现代的CPU 提供了多层TLB 缓存,就如同其他缓存一样;更高层的缓存更大也更慢.L1TLB 的小容量通常由令缓存为全关联式,加上LRU 逐出策略来弥补.近来,这种缓存的大小已经持续成长,并且––在进行中––被转变为集合关联式.因此,当一个新的项必须被加入时,被逐出并取代的项可能不是最旧的一个.

As noted above, the tag used to access the TLB is a part of the virtual address. If the tag has a match in the cache, the final physical address is computed by adding the page offset from the virtual address to the cached value. This is a very fast process; it has to be since the physical address must be available for every instruction using absolute addresses and, in some cases, for L2 look-ups which use the physical address as the key. If the TLB lookup misses the processor has to perform a page table walk; this can be quite costly.

如同上面所注记的,用来存取TLB 的标签为虚拟地址的一部分.若是在缓存中有比对到标签,最终的物理地址就能够由将来自虚拟地址的分页偏移量加到被缓存的值上而计算出来.这是个非常快的过程;它必须如此,因为物理地址必须可用于每条使用独立地址的指令,以及––在某些情况下––使用物理地址作为键值(key)的L2 查询.若是TLB 查询没有命中,处理器必须要进行一次页表走访;这可能是非常昂贵的.

Prefetching code or data through software or hardware could implicitly prefetch entries for the TLB if the address is on another page. This cannot be allowed for hardware prefetching because the hardware could initiate page table walks that are invalid. Programmers therefore cannot rely on hardware prefetching to prefetch TLB entries. It has to be done explicitly using prefetch instructions. TLBs, just like data and instruction caches, can appear in multiple levels. Just as for the data cache, the TLB usually appears in two flavors: an instruction TLB (ITLB) and a data TLB (DTLB). Higher-level TLBs such as the L2TLB are usually unified, as is the case with the other caches.

透过软件或硬件预取代码或数据时,若是地址在另一个分页上,能够暗自预取TLB 的项.这对于硬件预取而言是不可能的,因为硬件可能会引发无效的页表走访.程序员因而无法仰赖硬件预取来预取TLB 项.必须明确地使用预取指令来达成.TLB––就像是数据与指令缓存––能够出现在多个层级.就如同数据缓存一样,TLB 通常有两种:指令TLB(ITLB)以及数据TLB(DTLB).像是L2TLB 这种更高层级的TLB 通常是统一式的,与其它缓存的情况相同.

### 4.3.1. 使用TLB 的说明

The TLB is a processor-core global resource. All threads and processes executed on the processor core use the same TLB. Since the translation of virtual to physical addresses depends on which page table tree is installed, the CPU cannot blindly reuse the cached entries if the page table is changed. Each process has a different page table tree (but not the threads in the same process) as does the kernel and the VMM (hypervisor) if present. It is also possible that the address space layout of a process changes. There are two ways to deal with this problem:

TLB 是个处理器核心的全局(global)资源.所有执行在处理器核心的线程与进程都使用相同的TLB.由于虚拟到物理地址的翻译是看设置的是哪一个页表树而定的,因此若是页表被更改了,CPU 就不能盲目地重复使用缓存的项.每个进程有个不同的页表树(但同个进程中的线程并非如此).假如有的话,系统内核与VMM(虚拟机器监视器)亦是如此.一个进程的地址空间布局也是可能改变的.有两种处理这个问题的方式:

* The TLB is flushed whenever the page table tree is changed.
* 每当页表树被更改都刷新TLB.
* The tags for the TLB entries are extended to additionally and uniquely identify the page table tree they refer to.
* 扩展TLB 项的标签,以额外且唯一地识别它们所指向到的页表树.

In the first case the TLB is flushed whenever a context switch is performed. Since, in most OSes, a switch from one thread/process to another one requires executing some kernel code, TLB flushes are restricted to entering and leaving the kernel address space. On virtualized systems it also happens when the kernel has to call the VMM and on the way back. If the kernel and/or VMM does not have to use virtual addresses, or can reuse the same virtual addresses as the process or kernel which made the system/VMM call, the TLB only has to be flushed if, upon leaving the kernel or VMM, the processor resumes execution of a different process or kernel.

在第一种情况中,每当上下文切换(context switch)都会刷新TLB.由于––在大多OS 中––从一个线程／进程切换到另一个时,需要执行一些系统内核的代码,TLB 刷新会被限制在离开(有时候是进入)系统内核地址空间时.在虚拟化的系统上,当系统内核必须调用VMM,并在返回的途中时,这也会发生.若是系统内核和／或VMM 不必使用虚拟地址,或是能够重复使用与发出系统／VMM 调用的进程或系统内核相同的虚拟地址(即,地址空间被重叠了),TLB 只须在– –离开系统内核或VMM 后––处理器恢复一个不同的进程或系统内核的执行时刷新.

Flushing the TLB is effective but expensive. When executing a system call, for instance, the kernel code might be restricted to a few thousand instructions which touch, perhaps, a handful of new pages (or one huge page, as is the case for Linux on some architectures). This work would replace only as many TLB entries as pages are touched. For Intel's Core2 architecture with its 128 ITLB and 256 DTLB entries, a full flush would mean that more than 100 and 200 entries (respectively) would be flushed unnecessarily. When the system call returns to the same process, all those flushed TLB entries could be used again, but they will be gone. The same is true for often-used code in the kernel or VMM. On each entry into the kernel the TLB has to be filled from scratch even though the page tables for the kernel and VMM usually do not change and, therefore, TLB entries could, in theory, be preserved for a very long time. This also explains why the TLB caches in today's processors are not bigger: programs most likely will not run long enough to fill all these entries.

刷新TLB 有效但昂贵.举例来说,在执行一个系统调用时,系统内核程序可能会被限制在数千行触及––或许––少数的新分页(或者一个大分页,如同在某些架构上的Linux 的情况)的指令.这个操作仅会取代与被触及的分页一样多的TLB 项.以Intel 的Core2 架构,附加它的128 ITLB 与256 DTLB 的项而言,一次完整的刷新可能意味着被不必要地刷新的项(分别)会超过100 与200 个.当系统调用返回(return)到相同的进程时,所有那些被刷新的TLB 项都能够被再次用到,但它们将会被丢掉.对于在系统内核或VMM 中经常用到的代码亦是如此.尽管系统内核以及VMM 的页表通常不会改变,因此TLB 项––理论上––能够被保存相当长的一段时间,但在每次进入系统内核时,TLB 也必须从零开始填入.这也解释了为何现今处理器中的TLB 缓存并没有更大的原因:程序的执行时间非常可能不会长到足以填入这所有的项.

This fact, of course, did not escape the CPU architects. One possibility to optimize the cache flushes is to individually invalidate TLB entries. For instance, if the kernel code and data falls into a specific address range, only the pages falling into this address range have to evicted from the TLB. This only requires comparing tags and, therefore, is not very expensive. This method is also useful in case a part of the address space is changed, for instance, through a call to munmap.

这个事实––当然––不会逃出CPU 架构师的掌心.优化缓存刷新的一个可能性是,单独令TLB 项失效.举例来说,若是系统内核与数据落在一个特殊的地址范围,那么仅有落在这个地址范围的分页必须从TLB 逐出.这只需要比对标签,因而不怎么昂贵.这个方法在地址空间的一部分––例如,透过一次`munmap` 调用––被更改的情况下也是有用的.

A much better solution is to extend the tag used for the TLB access. If, in addition to the part of the virtual address, a unique identifier for each page table tree (i.e., a process's address space) is added, the TLB does not have to be completely flushed at all. The kernel, VMM, and the individual processes all can have unique identifiers. The only issue with this scheme is that the number of bits available for the TLB tag is severely limited, while the number of address spaces is not. This means some identifier reuse is necessary. When this happens the TLB has to be partially flushed (if this is possible). All entries with the reused identifier must be flushed but this is, hopefully, a much smaller set.

一个好得多的解法是扩展用来TLB 存取的标签.若是––除了虚拟地址的部分以外––为每个页表树(即,一个进程的地址空间)加上一个唯一的识别子(identifier),TLB 根本就不必完全刷新.系统内核,VMM,以及独立的进程全都能够拥有唯一的识别子.采用这个方案的唯一议题是,可用于TLB 标签的位数量会被严重地限制,而地址空间的数量则否.这表示是有必要重复使用某些识别子的.当这种情况发生时,TLB 必须被部分刷新(如果可能的话).所有带着被重复使用的识别子的项都必须被刷新,但这––但愿如此––是个非常小的集合.

This extended TLB tagging is of general advantage when multiple processes are running on the system. If the memory use (and hence TLB entry use) of each of the runnable processes is limited, there is a good chance the most recently used TLB entries for a process are still in the TLB when it gets scheduled again. But there are two additional advantages:

当多个进程执行在系统中时,这种扩展的TLB 标记在虚拟化的范围之外是有优势的.假如每个可执行进程的内存使用(是故TLB 项的使用)受限了,有个好机会是,当一个进程再次被调度时,它最近使用的TLB 项仍然在TLB 中.但还有两个额外的优点:

* Special address spaces, such as those used by the kernel and VMM, are often only entered for a short time; afterward control is often returned to the address space which initiated the call. Without tags, two TLB flushes are performed. With tags the calling address space's cached translations are preserved and, since the kernel and VMM address space do not often change TLB entries at all, the translations from previous system calls, etc. can still be used.
* When switching between two threads of the same process no TLB flush is necessary at all. Without extended TLB tags the entry into the kernel destroys the first thread's TLB entries, though.

1. 特殊的地址空间––像是那些被系统内核或VMM 所用到的––通常只会被进入一段很短的时间;后续的控制经常是返回到启动这次进入的地址空间.没有标签的话,便会执行一或两次TLB 刷新.有标签的话,调用地址空间的缓存翻译会被保留,而且––由于系统内核与VMM 地址空间根本不常更改TLB 项––来自前一次系统调用的翻译等仍然可以被使用.
2. 当在两条相同进程的线程之间切换时,根本不需要TLB 刷新.不过,没有扩展的TLB 标签的话,进入系统内核就会销毁第一条线程的项.

Some processors have, for some time, implemented these extended tags. AMD introduced a 1-bit tag extension with the Pacifica virtualization extensions. This 1-bit Address Space ID (ASID) is, in the context of virtualization, used to distinguish the VMM's address space from that of the guest domains. This allows the OS to avoid flushing the guest's TLB entries every time the VMM is entered (for instance, to handle a page fault) or the VMM's TLB entries when control returns to the guest. The architecture will allow the use of more bits in the future. Other mainstream processors will likely follow suit and support this feature.

某些处理器已经––一段时间了––实现了这些扩展标签.AMD 以Pacifica 虚拟化扩展引入了一种1 位的标签扩展.这个1 位地址空间ID(Address Space ID,ASID)是––在虚拟化的上下文中––用以从客户域(guest domain)的地址空间区别出VMM 的地址空间.这使得OS 得以避免在每次进入VMM(举例来说,处理一个分页错误[page fault])时刷新客户端的TLB 项,或者在返回客户端时刷新VMM 的TLB 项.这个架构未来将会允许使用更多的位.其它主流处理器可能也会遵循这套方法并支持这个功能.

### 4.3.2. 影响TLB 性能

There are a couple of factors which influence TLB performance. The first is the size of the pages. Obviously, the larger a page is, the more instructions or data objects will fit into it. So a larger page size reduces the overall number of address translations which are needed, meaning that fewer entries in the TLB cache are needed. Most architectures allow the use of multiple different page sizes; some sizes can be used concurrently. For instance, the x86/x86-64 processors have a normal page size of 4kB but they can also use 4MB and 2MB pages respectively. IA-64 and PowerPC allow sizes like 64kB as the base page size.

有几个影响TLB 性能的因素.第一个是分页的大小.显然地,分页越大,会被塞进去的指令或数据对象也越多.所以一个比较大的缓存大小减少了所需地址翻译的整体数量,代表TLB 缓存中需要更少的项.大部分架构现今允许使用多种不同的分页大小;有些大小能够并存地使用.举例来说,x86／x86-64 处理器拥有寻常的4kB 分页大小,但它们也分别能够使用4MB 与2MB 的分页.IA-64 与PowerPC 允许像是64kB 的大小作为基础分页大小.

The use of large page sizes brings some problems with it, though. The memory regions used for the large pages must be contiguous in physical memory. If the unit size for the administration of physical memory is raised to the size of the virtual memory pages, the amount of wasted memory will grow. All kinds of memory operations (like loading executables) require alignment to page boundaries. This means, on average, that each mapping wastes half the page size in physical memory for each mapping. This waste can easily add up; it thus puts an upper limit on the reasonable unit size for physical memory allocation.

不过,大分页尺寸的使用也随之带来了一些问题.为了大分页而使用的内存区域在物理内存中必须是连续的.若是物理内存管理的单位大小被提高到虚拟内存分页的大小,浪费的内存总量就会增加.各种内存操作(像是载入可执进程序)需要对齐到分页边界.这表示,平均而言,在每次映射的物理内存中,每次映射浪费了一半的分页大小.这种浪费能够轻易地累加;这因此对物理内存分配的合理单位大小加了个上限.

It is certainly not practical to increase the unit size to 2MB to accommodate large pages on x86-64. This is just too large a size. But this in turn means that each large page has to be comprised of many smaller pages. And these small pages have to be contiguous in physical memory. Allocating 2MB of contiguous physical memory with a unit page size of 4kB can be challenging. It requires finding a free area with 512 contiguous pages. This can be extremely difficult (or impossible) after the system runs for a while and physical memory becomes fragmented.

将单位大小提升到2MB,以容纳x86-64 上的大分页无疑并不实际.这个大小太大了.但这又意味着每个大分页必须由多个较小的分页所构成.而且这些小分页在物理内存(*physical memory*)中必须是连续的.以4kB 的单位分页大小分配2MB 的连续物理内存具有挑战性.这需要寻找一个有着512 个连续分页的空闲区域.在系统执行一段时间,并且物理内存变得片段之后,这可能极端困难(或者不可能).

On Linux it is therefore necessary to pre-allocate these big pages at system start time using the special hugetlbfs filesystem. A fixed number of physical pages are reserved for exclusive use as big virtual pages. This ties down resources which might not always be used. It also is a limited pool; increasing it normally means restarting the system. Still, huge pages are the way to go in situations where performance is a premium, resources are plenty, and cumbersome setup is not a big deterrent. Database servers are an example.

因此在Linux 上,有必要在系统启动的时候使用特殊的`hugetlbfs` 文件系统来分配这些大分页.一个固定数量的物理分页会被保留来专门作为大虚拟分页来使用.这绑住了可能不会一直用到的资源.这也是个有限的池(pool);增加它通常代表着重新启动系统.尽管如此,在性能贵重,资源充足,且麻烦的设置不是个大阻碍的情况下,庞大的分页便为大势所趋.数据库服务器就是个例子.

```shell

$ eu-readelf -l /bin/ls
Program Headers:
  Type Offset VirtAddr PhysAddr FileSiz MemSiz Flg Align
...
  LOAD 0x000000 0x0000000000400000 0x0000000000400000 0x0132ac 0x0132ac RE 0x200000
  LOAD 0x0132b0 0x00000000006132b0 0x00000000006132b0 0x001a71 0x001a71 RW 0x200000
...

```

Increasing the minimum virtual page size (as opposed to optional big pages) has its problems, too. Memory mapping operations (loading applications, for example) must conform to these page sizes. No smaller mappings are possible. The location of the various parts of an executable have, for most architectures, a fixed relationship. If the page size is increased beyond what has been taken into account when the executable or DSO was built, the load operation cannot be performed. It is important to keep this limitation in mind. Figure 4.3 shows how the alignment requirements of an ELF binary can be determined. It is encoded in the ELF program header.

提高最小的虚拟分页大小(对比于可选的大分页)也有它的问题.内存映射操作(例如,载入应用程序)必须遵循这些分页大小.不可能有更小的映射.一个可执进程序不同部分的位置––对大多架构而言––有个固定的关系.若是分页大小增加到超过在可执进程序或者DSO 创建时所考虑的大小时,就无法执行载入操作.将这个限制记在心上是很重要的.图4.3 显示了能够如何决定一个ELF 二进制数据(binary)的对齐需求的.它被编码在ELF 的程序标头(header).

In this example, an x86-64 binary, the value is 0x200000 = 2,097,152 = 2MB which corresponds to the maximum page size supported by the processor.

在这个例子中,一个x86-64 的二进制数据,值为 0x200000 = 2,097,152 = 2MB,与处理器所支持的最大分页大小相符.

There is a second effect of using larger page sizes: the number of levels of the page table tree is reduced. Since the part of the virtual address corresponding to the page offset increases, there are not that many bits left which need to be handled through page directories. This means that, in case of a TLB miss, the amount of work which has to be done is reduced.

使用较大的分页大小有个次要的影响:页表树的层级数量会被减低.由于对应到分页偏移量的虚拟地址部分增加了,就没有剩下那么多需要透过分页目录处理的位了.这表示,在一次TLB 不命中的情况下,必须完成的工作总量减少了.

Beyond using large page sizes, it is possible to reduce the number of TLB entries needed by moving data which is used at the same time to fewer pages. This is similar to some optimizations for cache use we talked about above. Only now the alignment required is large. Given that the number of TLB entries is quite small this can be an important optimization.

除了使用大分页尺寸外,也可能由将同时用到的数据搬移到较少的分页上,以减少所需的TLB 项数量.这类似于我们先前讨论的针对缓存使用的一些优化.考虑到TLB 项的数量非常少,这会是个重要的优化.

## 4.4. 虚拟化的影响

Virtualization of OS images will become more and more prevalent; this means another layer of memory handling is added to the picture. Virtualization of processes (basically jails) or OS containers do not fall into this category since only one OS is involved. Technologies like Xen or KVM enable—with or without help from the processor—the execution of independent OS images. In these situations there is one piece of software alone which directly controls access to the physical memory.

OS 镜像(image)的虚拟化会变得越来越流行;这表示内存管理的另一层会被加到整体中.进程(基本上为监狱[jail])或OS 容器(container)的虚拟化并不属于这个范畴,因为只有一个OS 会牵涉其中.像Xen 或KVM 这类技术能够––无论有没有来自处理器的协助––执行独立OS 镜像.在这些情况下,只有一个直接控制物理内存存取的软件.

![图4.4:Xen 虚拟化模型](assets/figure-4.4.png)

In the case of Xen (see Figure 4.4) the Xen VMM is that piece of software. The VMM does not implement many of the other hardware controls itself, though. Unlike VMMs on other, earlier systems (and the first release of the Xen VMM) the hardware outside of memory and processors is controlled by the privileged Dom0 domain. Currently, this is basically the same kernel as the unprivileged DomU kernels and, as far as memory handling is concerned, they do not differ. Important here is that the VMM hands out physical memory to the Dom0 and DomU kernels which, themselves, then implement the usual memory handling as if they were running directly on a processor.

在Xen 的情况下(见图4.4),Xen VMM 即是这个软件.不过VMM 本身并不实现太多其它的硬件控制.不若在其它较早期的系统(以及首次释出的Xen VMM)上的VMM,除了内存与处理器之外的硬件是由具有特权的Dom0 域所控制的.目前,这基本上是与没有特权的DomU 系统内核相同的系统内核,而且––就所关心的内存管理而言––它们没什么区别.重要的是,VMM 将物理内存分发给了Dom0 与DomU 系统内核,其因而实现了普通的内存管理,就好像它们是直接执行在一个处理器上一样.

To implement the separation of the domains which is required for the virtualization to be complete, the memory handling in the Dom0 and DomU kernels does not have unrestricted access to physical memory. The VMM does not hand out memory by giving out individual physical pages and letting the guest OSes handle the addressing; this would not provide any protection against faulty or rogue guest domains. Instead, the VMM creates its own page table tree for each guest domain and hands out memory using these data structures. The good thing is that access to the administrative information of the page table tree can be controlled. If the code does not have appropriate privileges it cannot do anything.

为了实现完成虚拟化所需的域的分离,Dom0 与DomU 系统内核中的内存处理并不具有无限制的物理内存存取.VMM 不是由分发独立的物理分页,并让客户端OS 处理定址的方式来分发内存;这不会提供任何针对有缺陷或者流氓客户域的防范.取而代之地,VMM 会为每个客户域建立它自己拥有的页表树,并使用这些数据结构来分发内存.好处是能够控制对页表树的管理资讯的存取.若是程序没有合适的权限,它就什么也无法做.

This access control is exploited in the virtualization Xen provides, regardless of whether para- or hardware (aka full) virtualization is used. The guest domains construct their page table trees for each process in a way which is intentionally quite similar for para- and hardware virtualization. Whenever the guest OS modifies its page tables the VMM is invoked. The VMM then uses the updated information in the guest domain to update its own shadow page tables. These are the page tables which are actually used by the hardware. Obviously, this process is quite expensive: each modification of the page table tree requires an invocation of the VMM. While changes to the memory mapping are not cheap without virtualization they become even more expensive now.

这种存取控制被利用在Xen 提供的虚拟化之中,无论使用的是半虚拟化(paravirtualization)或是硬件虚拟化(亦称全虚拟化).客户域采用了有意与半虚拟化以及硬件虚拟化十分相似的方式,为每个进程建立了它们的页表树.无论客户端OS 在何时修改了它的页表,都会调用VMM.VMM 于是使用在客户域中更新的资讯来更新它自己拥有的影子页表.这些是实际被硬件用到的页表.显然地,这个过程相当昂贵:页表树每次修改都需要一次VMM 的调用.在没有虚拟化的情况下对内存映射的更动并不便宜,而它们现在甚至变得更昂贵了.

The additional costs can be really large, considering that the changes from the guest OS to the VMM and back themselves are already quite expensive. This is why the processors are starting to have additional functionality to avoid the creation of shadow page tables. This is good not only because of speed concerns but it also reduces memory consumption by the VMM. Intel has Extended Page Tables (EPTs) and AMD calls it Nested Page Tables (NPTs). Basically both technologies have the page tables of the guest OSes produce virtual physical addresses. These addresses must then be further translated, using the per-domain EPT/NPT trees, into actual physical addresses. This will allow memory handling at almost the speed of the no-virtualization case since most VMM entries for memory handling are removed. It also reduces the memory use of the VMM since now only one page table tree for each domain (as opposed to process) has to be maintained.

考虑到从客户端OS 到VMM 的更改并返回,它们本身已经非常昂贵,额外的成本可能非常大.这即是为何处理器开始拥有额外的功能,以避免影子页表的建立.这很好,不仅因为速度的关系,它也减少了VMM 的内存消耗.Intel 有扩展页表(Extended Page Table,EPT),而AMD 称它为巢状页表(Nested Page Table,NPT).基本上这两个技术都拥有客户端OS 从「客户虚拟地址(guest virtual address)」产生「宿主虚拟地址(host virtual address)」的页表.宿主虚拟地址接着必须被进一步––使用每个域的EPT／NPT 树––翻译成真正的物理地址.这会令内存处理以几乎是非虚拟化情况的速度来进行,因为大多数内存管理的VMM 项​​目都被移除了.它也减少了VMM 的内存使用,因为现在每个域(对比于进程)都仅有一个必须要维护的分页.

The results of the additional address translation steps are also stored in the TLB. That means the TLB does not store the virtual physical address but, instead, the complete result of the lookup. It was already explained that AMD's Pacifica extension introduced the ASID to avoid TLB flushes on each entry. The number of bits for the ASID is one in the initial release of the processor extensions; this is just enough to differentiate VMM and guest OS. Intel has virtual processor IDs (VPIDs) which serve the same purpose, only there are more of them. But the VPID is fixed for each guest domain and therefore it cannot be used to mark separate processes and avoid TLB flushes at that level, too.

这个额外的地址翻译步骤的结果也会储存在TLB 中.这表示TLB 不会储存虚拟的物理地址,而是查询的完整结果.已经解释过AMD 的Pacifica 扩展引入了ASID 以避免在每个项上的TLB 刷新.ASID 的位数量在最初释出的处理器扩展中只有一位;这足以区隔VMM 与客户端OS 了.Intel 拥有用于相同目的的虚拟处理器ID(virtual processor ID,VPID),只不过有更多的位数.但是对于每个客户域而言,VPID 都是固定的,因此它无法被用来标记个别的进程,也不能在这个层级避免TLB 刷新.

The amount of work needed for each address space modification is one problem with virtualized OSes. There is another problem inherent in VMM-based virtualization, though: there is no way around having two layers of memory handling. But memory handling is hard (especially when taking complications like NUMA into account, see Section 5). The Xen approach of using a separate VMM makes optimal (or even good) handling hard since all the complications of a memory management implementation, including “trivial” things like discovery of memory regions, must be duplicated in the VMM. The OSes have fully-fledged and optimized implementations; one really wants to avoid duplicating them.

每次地址空间修改所需的工作量是有着虚拟化OS 的一个问题.不过,基于VMM 的虚拟化还有另一个固有的问题:没有办法拥有两层内存处理.但是内存处理很难(尤其在将像NUMA 这类难题纳入考虑的时候,见第五节).Xen 使用一个分离VMM 的方式使得优化的(甚至是好的)处理变得困难,因为所有的内存管理实现的难题––包含像内存区域的探寻这类「琐碎」事––都必须在VMM 中重复.OS 拥有成熟且优化的实现;真的应该避免重复这些事.

![图4.5:KVM 虚拟化模型](assets/figure-4.5.png)

This is why carrying the VMM/Dom0 model to its conclusion is such an attractive alternative. Figure 4.5 shows how the KVM Linux kernel extensions try to solve the problem. There is no separate VMM running directly on the hardware and controlling all the guests; instead, a normal Linux kernel takes over this functionality. This means the complete and sophisticated memory handling functionality in the Linux kernel is used to manage the memory of the system. Guest domains run alongside the normal user-level processes in what the creators call “guest mode”. The virtualization functionality, para- or full virtualization, is controlled by another user-level process, the KVM VMM. This is just another process which happens to control a guest domain using the special KVM device the kernel implements.

这即是为何废除VMM／Dom0 模型是个如此有吸引力的替代方案.图4.5 显示了KVM Linux 系统内核扩展是如何试着解决这个问题的.没有直接执行在硬件上,并控制所有客户的分离VMM;而是一个普通的Linux 系统内核接管了这个功能.这表示在Linux 系统内核上完整且精密的内存处理功能被用来管理系统中的内存.客户域与被创造者称为「客户模式(guest mode)」的普通的用户层级进程一同执行.虚拟化功能––半虚拟化或全虚拟化––是由KVM VMM 所控制.这只不过是另一个用户层级的进程,使用系统内核实现的特殊KVM 设备来控制一个客户域.

The benefit of this model over the separate VMM of the Xen model is that, even though there are still two memory handlers at work when guest OSes are used, there only needs to be one implementation, that in the Linux kernel. It is not necessary to duplicate the same functionality in another piece of code like the Xen VMM. This leads to less work, fewer bugs, and, perhaps, less friction where the two memory handlers touch since the memory handler in a Linux guest makes the same assumptions as the memory handler in the outer Linux kernel which runs on the bare hardware.

这个模型相较于Xen 模型的分离VMM 的优点是,即使在使用客户端OS 时仍然有两个运作的内存处理者,但只需要唯一一种在Linux 系统内核中的实现.没有必要像Xen VMM 一样在另一段代码中重复相同的功能.这导致更少的工作,更少的臭虫,以及––也许––更少两个内存管理者接触的摩擦,因为在一个Linux 客户端中的内存管理者会与外部在裸机上执行的Linux 系统内核的内存管理者做出相同的假设.

Overall, programmers must be aware that, with virtualization used, the cost of memory operations is even higher than without virtualization. Any optimization which reduces this work will pay off even more in virtualized environments. Processor designers will, over time, reduce the difference more and more through technologies like EPT and NPT but it will never completely go away.

总而言之,程序员必须意识到,采用虚拟化的时候,缓存不命中(指令,数据,或TLB)的成本甚至比起没有虚拟化还要高.任何减少这些工作的优化,在虚拟化的环境中甚至会获得更多的回报.处理器设计者将会––随着时间的推移––透过像是EPT 与NPT 这类技术来逐渐减少这个差距,但它永远也不会完全消失.

## 注释

### 24

{Segment limits on x86 are performance-relevant but that is another story.}

x86 上的分段限制是性能相关的,但这又是另一个故事了.